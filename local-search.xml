<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深入kube-proxy ipvs模式的conn_reuse_mode问题</title>
    <link href="/2021/01/15/%E6%B7%B1%E5%85%A5kube-proxy%20ipvs%E6%A8%A1%E5%BC%8F%E7%9A%84conn_reuse_mode%E9%97%AE%E9%A2%98/"/>
    <url>/2021/01/15/%E6%B7%B1%E5%85%A5kube-proxy%20ipvs%E6%A8%A1%E5%BC%8F%E7%9A%84conn_reuse_mode%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>在高并发、短连接的场景下，kube-proxy ipvs存在rs删除失败或是延迟高的问题，社区也有不少Issue反馈，比如<a href="https://github.com/kubernetes/kubernetes/issues/81775">kube-proxy ipvs conn_reuse_mode setting causes errors with high load from single client</a>。文本对这些问题进行了梳理，试图介绍产生这些问题的内部原因。由于能力有限，其中涉及内核部分，只能浅尝辄止。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="端口重用"><a href="#端口重用" class="headerlink" title="端口重用"></a>端口重用</h3><p>一切问题来源于端口重用。在TCP四次挥手中有个<code>TIME_WAIT</code>的状态，作为先发送<code>FIN</code>包的一端，在接收到对端发送的<code>FIN</code>包后进入<code>TIME_WAIT</code>，在经过<code>2MSL</code>后才会真正关闭连接。<code>TIME_WAIT</code>状态的存在，一来可以避免将之前连接的延迟报文，作为当前连接的报文处理；二是可以处理最后一个ACK丢失带来的问题。</p><p><img src="/img/2021-01-15-TCP.png"></p><p>而在短连接、高并发的场景下，会出现大量的<code>TIME-WAIT</code>连接，导致资源无法及时释放。Linux中内核参数<code>net.ipv4.tcp_tw_reuse</code>提供了一种减少<code>TIME-WAIT</code>连接的方式，可以将<code>TIME-WAIT</code>连接的端口分配给新的TCP连接，来复用端口。</p><pre><code class="hljs text">tcp_tw_reuse - BOOLEANAllow to reuse TIME-WAIT sockets for new connections when it issafe from protocol viewpoint. Default value is 0.It should not be changed without advice/request of technicalexperts.</code></pre><h3 id="ipvs如何处理端口重用？"><a href="#ipvs如何处理端口重用？" class="headerlink" title="ipvs如何处理端口重用？"></a>ipvs如何处理端口重用？</h3><p>ipvs对端口的复用策略主要由内核参数<code>net.ipv4.vs.conn_reuse_mode</code>决定</p><pre><code class="hljs text">conn_reuse_mode - INTEGER1 - defaultControls how ipvs will deal with connections that are detectedport reuse. It is a bitmap, with the values being:0: disable any special handling on port reuse. The newconnection will be delivered to the same real server that wasservicing the previous connection. This will effectivelydisable expire_nodest_conn.bit 1: enable rescheduling of new connections when it is safe.That is, whenever expire_nodest_conn and for TCP sockets, whenthe connection is in TIME_WAIT state (which is only possible ifyou use NAT mode).bit 2: it is bit 1 plus, for TCP connections, when connectionsare in FIN_WAIT state, as this is the last state seen by loadbalancer in Direct Routing mode. This bit helps on adding newreal servers to a very busy cluster.</code></pre><p>当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上；当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，ipvs则会对新连接进行重新调度。</p><p>相关的，还有一个内核参数<code>net.ipv4.vs.expire_nodest_conn</code>，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包<strong>silently drop</strong>，也就是DROP掉数据包但不结束连接，等待客户端的重试。</p><p>另外，关于<strong>destination 不可用</strong>的判断，是在ipvs执行删除<code>vs</code>（在<code>__ip_vs_del_service()</code>中实现）或删除<code>rs</code>（在<code>ip_vs_del_dest()</code>中实现）时，会调用<code>__ip_vs_unlink_dest()</code>方法，将相应的destination置为不可用。</p><pre><code class="hljs text">expire_nodest_conn - BOOLEAN        0 - disabled (default)        not 0 - enabled        The default value is 0, the load balancer will silently drop        packets when its destination server is not available. It may        be useful, when user-space monitoring program deletes the        destination server (because of server overload or wrong        detection) and add back the server later, and the connections        to the server can continue.        If this feature is enabled, the load balancer will expire the        connection immediately when a packet arrives and its        destination server is not available, then the client program        will be notified that the connection is closed. This is        equivalent to the feature some people requires to flush        connections when its destination is not available.</code></pre><p>  关于ipvs如何处理端口复用的连接，这块主要实现逻辑在<code>net/netfilter/ipvs/ip_vs_core.c</code>的<code>ip_vs_in()</code>方法中：</p><pre><code class="hljs C"><span class="hljs-comment">/*</span><span class="hljs-comment"> * Check if the packet belongs to an existing connection entry</span><span class="hljs-comment"> */</span>cp = pp-&gt;conn_in_get(ipvs, af, skb, &amp;iph);  <span class="hljs-comment">//找是属于某个已有的connection</span>conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);<span class="hljs-comment">//当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的connection），进行处理</span><span class="hljs-keyword">if</span> (conn_reuse_mode &amp;&amp; !iph.fragoffs &amp;&amp; is_new_conn(skb, &amp;iph) &amp;&amp; cp) &#123; <span class="hljs-keyword">bool</span> uses_ct = <span class="hljs-literal">false</span>, resched = <span class="hljs-literal">false</span>;<span class="hljs-comment">//如果开启了expire_nodest_conn、目标rs的weight为0</span><span class="hljs-keyword">if</span> (unlikely(sysctl_expire_nodest_conn(ipvs)) &amp;&amp; cp-&gt;dest &amp;&amp;    unlikely(!atomic_read(&amp;cp-&gt;dest-&gt;weight))) &#123;resched = <span class="hljs-literal">true</span>;<span class="hljs-comment">//查询是否用到了conntrack</span>uses_ct = ip_vs_conn_uses_conntrack(cp, skb);&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (is_new_conn_expected(cp, conn_reuse_mode)) &#123;<span class="hljs-comment">//连接是expected的情况，比如FTP</span>uses_ct = ip_vs_conn_uses_conntrack(cp, skb);<span class="hljs-keyword">if</span> (!atomic_read(&amp;cp-&gt;n_control)) &#123;resched = <span class="hljs-literal">true</span>;&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">/* Do not reschedule controlling connection</span><span class="hljs-comment"> * that uses conntrack while it is still</span><span class="hljs-comment"> * referenced by controlled connection(s).</span><span class="hljs-comment"> */</span>resched = !uses_ct;&#125;&#125;<span class="hljs-comment">//如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了</span><span class="hljs-keyword">if</span> (resched) &#123;<span class="hljs-keyword">if</span> (!atomic_read(&amp;cp-&gt;n_control))ip_vs_conn_expire_now(cp);__ip_vs_conn_put(cp);<span class="hljs-comment">//当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN</span><span class="hljs-keyword">if</span> (uses_ct)<span class="hljs-keyword">return</span> NF_DROP;<span class="hljs-comment">//未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程</span>cp = <span class="hljs-literal">NULL</span>;&#125;&#125;<span class="hljs-keyword">if</span> (unlikely(!cp)) &#123;<span class="hljs-keyword">int</span> v;<span class="hljs-keyword">if</span> (!ip_vs_try_to_schedule(ipvs, af, skb, pd, &amp;v, &amp;cp, &amp;iph))<span class="hljs-keyword">return</span> v;&#125;IP_VS_DBG_PKT(<span class="hljs-number">11</span>, af, pp, skb, iph.off, <span class="hljs-string">&quot;Incoming packet&quot;</span>);<span class="hljs-comment">/* Check the server status */</span><span class="hljs-keyword">if</span> (cp-&gt;dest &amp;&amp; !(cp-&gt;dest-&gt;flags &amp; IP_VS_DEST_F_AVAILABLE)) &#123;<span class="hljs-comment">/* the destination server is not available */</span>__u32 flags = cp-&gt;flags;<span class="hljs-comment">/* when timer already started, silently drop the packet.*/</span><span class="hljs-keyword">if</span> (timer_pending(&amp;cp-&gt;timer))__ip_vs_conn_put(cp);<span class="hljs-keyword">else</span>ip_vs_conn_put(cp);<span class="hljs-keyword">if</span> (sysctl_expire_nodest_conn(ipvs) &amp;&amp;    !(flags &amp; IP_VS_CONN_F_ONE_PACKET)) &#123;<span class="hljs-comment">/* try to expire the connection immediately */</span>ip_vs_conn_expire_now(cp);&#125;<span class="hljs-keyword">return</span> NF_DROP;&#125;</code></pre><h3 id="kube-proxy-ipvs模式下的优雅删除"><a href="#kube-proxy-ipvs模式下的优雅删除" class="headerlink" title="kube-proxy ipvs模式下的优雅删除"></a>kube-proxy ipvs模式下的优雅删除</h3><p>Kubernetes提供了Pod优雅删除机制。当我们决定干掉一个Pod时，我们可以通过<code>PreStop Hook</code>来做一些服务下线前的处理，同时Kubernetes也有个<code>grace period</code>，超过这个时间但未完成删除的Pod会被强制删除。</p><p>而在Kubernetes 1.13之前，kube-proxy ipvs模式并不支持优雅删除，当Endpoint被删除时，kube-proxy会直接移除掉ipvs中对应的rs，这样会导致后续的数据包被丢掉。</p><p>在1.13版本后，Kubernetes添加了<a href="https://github.com/kubernetes/kubernetes/pull/66012">IPVS优雅删除</a>的逻辑，主要是两点：</p><ul><li>当Pod被删除时，kube-proxy会先将rs的<code>weight</code>置为0，以防止新连接的请求发送到此rs，由于不再直接删除rs，旧连接仍能与rs正常通信；</li><li>当rs的<code>ActiveConn</code>数量为0（后面版本已改为<code>ActiveConn+InactiveConn==0</code>)，即不再有连接转发到此rs时，此rs才会真正被移除。</li></ul><h2 id="kube-proxy-ipvs模式下的问题"><a href="#kube-proxy-ipvs模式下的问题" class="headerlink" title="kube-proxy ipvs模式下的问题"></a>kube-proxy ipvs模式下的问题</h2><p>看上去kube-proxy ipvs的删除是优雅了，但当优雅删除正巧碰到端口重用，那问题就来了。</p><p>首先，kube-proxy希望通过设置<code>weight</code>为0，来避免新连接转发到此rs。但当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，对于端口复用的连接，ipvs不会主动进行新的调度（调用<code>ip_vs_try_to_schedule</code>方法）；同时，只是将<code>weight</code>置为0，也并不会触发由<code>expire_nodest_conn</code>控制的结束连接或DROP操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的Pod。这样一来，只要不断的有端口复用的连接请求发来，rs就不会被kube-proxy删除，上面提到的优雅删除的两点均无法实现。</p><p>而当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，根据<code>ip_vs_in()</code>的处理逻辑，当开启了<code>net.ipv4.vs.conntrack</code>时，会DROP掉第一个SYN包，导致SYN的重传，有1S延迟。而Kube-proxy在IPVS模式下，使用了iptables进行<code>MASQUERADE</code>，也正好开启了<code>net.ipv4.vs.conntrack</code>。</p><pre><code class="hljs text">conntrack - BOOLEAN0 - disabled (default)not 0 - enabledIf set, maintain connection tracking entries forconnections handled by IPVS.This should be enabled if connections handled by IPVS are to bealso handled by stateful firewall rules. That is, iptables rulesthat make use of connection tracking.  It is a performanceoptimisation to disable this setting otherwise.Connections handled by the IPVS FTP application modulewill have connection tracking entries regardless of this setting.Only available when IPVS is compiled with CONFIG_IP_VS_NFCT enabled.</code></pre><p>这样看来，目前的情况似乎是，如果你需要实现优雅删除中的“保持旧连接不变，调度新连接”能力，那就要付出1s的延迟代价；如果你要好的性能，那么就不能重新调度。</p><h2 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h2><p>从Kubernetes角度来说，Kube-proxy需要在保证性能的前提下，找到一种能让新连接重新调度的方式。但目前从内核代码中可以看到，需要将参数设置如下</p><pre><code class="hljs text">net.ipv4.vs.conntrack=0net.ipv4.vs.conn_reuse_mode=1net.ipv4.vs.expire_nodest_conn=1</code></pre><p>但Kube-proxy ipvs模式目前无法摆脱iptables来完成k8s service的转发。此外，Kube-proxy只有在<code>ActiveConn+InactiveConn==0</code>时才会删除rs，除此之外，在新的Endpoint和<code>GracefulTerminationList</code>（保存了<code>weight</code>为0，但暂未删除的rs）中的rs冲突时，才会立即删除rs。这种逻辑似乎并不合理。目前Pod已有优雅删除的逻辑，而kube-proxy应基于Pod的优雅删除，在网络层面做好rs的优雅删除，因此在kubelet完全删除Pod后，Kube-proxy是否也应该考虑同时删除相应的rs？</p><p>另外，从内核角度来说，ipvs需要提供一种方式，能在端口复用、同时使用conntrack的场景下，可以对新连接直接重新调度。</p><h2 id="即将到来"><a href="#即将到来" class="headerlink" title="即将到来"></a>即将到来</h2><p>这个问题在社区讨论一段时间后，目前出现的几个相关的解决如下：<br><strong>内核两个Patch</strong></p><ul><li><a href="http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200701151719.4751-1-ja@ssi.bg/">ipvs: allow connection reuse for unconfirmed conntrack</a><br>修改了<code>ip_vs_conn_uses_conntrack()</code>方法的逻辑，当使用<code>unconfirmed conntrack</code>时，返回false，这种修改针对了TIME_WAIT的conntrack。</li><li><a href="http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200708161638.13584-1-kim.andrewsy@gmail.com/"> ipvs: queue delayed work to expire no destination connections if expire_nodest_conn=1</a><br>提前了<code>expire  connection</code>的操作，在destination被删除后，便开始将<code>expire  connection</code>操作入队列。而不是等到数据包真正发过来时，才做<code>expire  connection</code>，以此来减少数据包的丢失。</li></ul><p><strong>Kubernetes</strong><br><a href="https://github.com/kubernetes/enhancements/pull/1607">Graceful Termination for External Traffic Policy Local</a><br><a href="https://github.com/kubernetes/kubernetes/pull/92968">Add Terminating Condition to EndpointSlice</a><br>正如前面所说的，Kube-proxy需要能够感知到Pod的优雅删除过程，来同步进行rs的删除。目前，已有一个相应的KEP在进行中，通过在<code>Endpoint.EndpointConditions</code>中添加<code>terminating</code>字段，来为kube-proxy提供感知方式。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kube-proxy</tag>
      
      <tag>ipvs</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
