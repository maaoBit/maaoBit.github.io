<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>廉价的K8S LB方案--MetalLB</title>
    <link href="/2021/02/09/%E5%BB%89%E4%BB%B7%E7%9A%84K8S-LB%E6%96%B9%E6%A1%88-MetalLB/"/>
    <url>/2021/02/09/%E5%BB%89%E4%BB%B7%E7%9A%84K8S-LB%E6%96%B9%E6%A1%88-MetalLB/</url>
    
    <content type="html"><![CDATA[<p>MetalLB为本地运行的Kubernetes集群提供了LoadBalance功能，使用户像在公有云环境一样，使用K8S的LoadBalancer Service。</p><a id="more"></a><p>在Kubernetes中部署MetalLB后，会有两个部分，一部分是负责监听K8S Service资源变化的Controller，以Deployment的方式部署在整个集群上，另一部分是各个节点上的代理组件Speaker，以DaemonSet方式部署，负责ARP Reply或是BGP的宣告。</p><p>MetalLB有两种模式，一种是二层网络模式，一种是BGP模式。</p><h2 id="二层网络模式"><a href="#二层网络模式" class="headerlink" title="二层网络模式"></a>二层网络模式</h2><p><img src="/img/metallb1.png"></p><p>二层网络模式，顾名思义，所有的流量都在一个二层网络中。此模式下MetalLB其实并未进行负载均衡，而是借助其他组件例如Kube-proxy，来实现负载均衡。具体的，如上图所示，MetalLB将Service External IP（由MetalLB的Controller根据预先设定好的IP段，配置在LoadBalance类型的Service上）配置在K8S集群节点的local网卡上（比如kube-ipvs0），当Client访问Service External IP时，由于在同一个二层，会广播ARP请求，MetalLB会使用某个节点进行ARP Reply（IPv6通过NDP协议），从而Client请求流量会发送到此节点，然后由节点上的Kube-proxy进行负载，转到真正的Pod地址。</p><p>需要说明的是：</p><p>1）Service External IP地址需要与Client在同一网段。</p><p>2）为了使其他节点不对local网卡上的Service External IP进行ARP Reply，节点需要设置<code>arp_ignore=1</code>以及<code>arp_announce=2</code>，或者是设置Kube-proxy的<code>--ipvs-strict-arp</code>参数。</p><p>3）MetalLB对于每个Service会选一个节点，始终由这个节点进行ARP Reply。节点的选择方式是：先过滤出Service后端Pod所在的节点，然后以Service Name、Service Namespace、NodeIP等计算hash值，排序hash值取第一个。</p><p>3）由于上面的选择方式，MetalLB对ExternalTrafficPolicy=Local的Service是支持的，但会导致只用到部分后端Pod实例。</p><p>4）Kube-proxy会同时进行DNAT与SNAT，回包并不是如LVS DR模式，而是原路返回。</p><p>5）Kube-proxy也可以使用其他组件代替，比如Cilium的Kube-proxy replace方案。</p><p>二层网络模式在生产环境中使用有限，原因有二：</p><p>1）扩展性有限。受ARP协议和NDP协议的限制，每个Service都只有一个真正的“入口”，因此这个节点很可能会成为网络瓶颈的所在。</p><p>2）故障转移依赖客户端。MetalLB使用<a href="https://github.com/hashicorp/memberlist">memberlist</a>做为分布式节点的管理，如果当前使用的节点出现故障时，MetalLB会在新的memberlist中（已删除故障节点）再次选择某个节点进行替换。接着MetalLB会给客户端发送一个“额外的二层包”，告知客户端OS需要更新他们的MAC缓存，而在客户端OS更新缓存前，流量仍会转发到故障节点。因此从某种程度来说，故障转移的时间，就依赖于客户端OS更新MAC缓存的速度。</p><p>3）选节点的hash并非一致性，添加节点有一定概率导致Service的节点发生变化。</p><h2 id="BGP模式"><a href="#BGP模式" class="headerlink" title="BGP模式"></a>BGP模式</h2><p><img src="/img/metallb2.png"></p><p>BGP模式不限于一个二层网络里，各个节点都会与交换机建立BGP Peer，宣告Service External IP的下一跳为自身，这样通过ECMP实现了一层负载。客户端请求通过交换机负载到后端某个节点后，再由Kube-proxy进行转发。</p><p>对于<code>externalTrafficPolicy=Local</code>的Service，只有本机存在服务Pod时，才会进行Service External IP的BGP路由宣告。</p><p><img src="/img/MetalLB3.png"></p><p>可以看到这种方式实际上是经过了两层的转发，当Service ExternalTrafficPolicy为Cluster时，每次转发的概率其实是均等的，但当Service ExternalTrafficPolicy为Local时，虽然转发到每个节点的概率是一致的，但每个节点上服务Pod数量不一致，导致每个Pod上的流量其实并不均等。</p><p>在这种模式中，我们希望对于客户端请求的转发遵循会话保持，否则会出现数据包的乱序或丢弃。一般在硬件上，通过对数据包的3元组（源地址、目标地址、协议）或是五元组（三元组加上源端口、目标端口）进行hash，来实现同一会话转发到相同的后端。但需要注意，这种hash一般不是一致性hash，这就导致当后端某个节点的失效后，会对其他连接也会参数影响，扩大了故障的“爆炸半径”。</p><p>官方给了一些缓解的办法，比如在MetalLB和Service后端之间，加入一层有状态的负载——ingress Controller。</p><p>另外 ，BGP模式下，MetalLB提供了部分参数，来实现对BGP协议的控制，比如BGP community、localpref等。</p><pre><code class="hljs routeros">apiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    peers:    - peer-address: 10.0.0.1      peer-asn: 64501      my-asn: 64500    address-pools:    - name: default      protocol: bgp      addresses:      - 198.51.100.0/24      bgp-advertisements:      - aggregation-length: 32        localpref: 100        communities:        - no-advertise      - aggregation-length: 24    bgp-communities:      no-advertise: 65535:65282</code></pre><h3 id="类比Calico对Service的BGP"><a href="#类比Calico对Service的BGP" class="headerlink" title="类比Calico对Service的BGP"></a>类比Calico对Service的BGP</h3><p>从Calico v3.4开始，Calico支持对K8S的Service地址进行BGP，其逻辑和MetalLB BGP逻辑基本相同。有几点区别：</p><p>1）默认的情况下，MetalLB是对单个IP进行BGP的，子网掩码是32，但对Calico配置的是一个CIDR，Calico对外BGP的是整个网络段的路由，只有在externalTrafficPolicy为Local时，才会对单个IP进行BGP。</p><p>2）Calico不仅会BGP Service External IP，还可以BGP Service ClusterIP，两者的转发过程是类似的，都是将流量负载到某个K8S节点后，进行SNAT与DNAT。</p><p>3）Calico不负责LoadBalancer Service IP的分配。</p><h2 id="IP地址共享"><a href="#IP地址共享" class="headerlink" title="IP地址共享"></a>IP地址共享</h2><p>默认情况下，MetalLB只会将一个IP地址分配到一个LoadBalancer Service上，用户可以通过<code>spec.loadBalancerIP</code>来指定自己想用的IP，如果用户指定了已被分配了的IP会，则会报错。但MetalLB也提供了方式去支持多个Service共享相同的IP，主要为了解决：K8S不支持对LoadBalancer Service中的Port指定多协议；有限的IP地址资源。</p><p>具体的方式是：创建两个Service，并加上<code>metallb.universe.tf/allow-shared-ip</code>为Key的<code>annotation</code>，表明Service能容忍使用共享的LoadBalancerIP；然后通过<code>spec.loadBalancerIP</code>给两个Service指定共享的IP。</p><p>IP地址共享也有限制：</p><p>1）两个Service的<code>metallb.universe.tf/allow-shared-ip</code>值是一样的。</p><p>2）两个Service的“端口”（带协议）不同，比如<code>tcp/53</code>和<code>udp/53</code>是属于不同的“端口”。</p><p>3）两个Service对应的后端Pod要一致，如果不一致，那么他们的externalTrafficPolicy需要都是Cluster，不然会无法进行正确的BGP。</p><p>目前K8S已经开始支持<a href="https://github.com/kubernetes/kubernetes/pull/94028">对LoadBalancer Service指定多协议</a>，因此除了MetalLB提供的IP地址共享的方式，也可以使用原生的功能。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://metallb.universe.tf/">https://metallb.universe.tf/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>metalLB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Calico BGP功能介绍：BIRD简介</title>
    <link href="/2021/01/26/Calico-BGP%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%EF%BC%9ABIRD%E7%AE%80%E4%BB%8B/"/>
    <url>/2021/01/26/Calico-BGP%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%EF%BC%9ABIRD%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<p>Calico作为一种常用的Kubernetes网络插件，使用BGP协议对各节点的容器网络进行路由交换。本文是《Calico BGP功能介绍》系列的第一篇，介绍Calico所使用的BGP软件路由器——BIRD。</p><a id="more"></a><blockquote><p>关于BGP协议，网上资料众多，在这里不再做介绍。另外，推荐《BGP in the datacenter》作为BGP应用的进阶阅读，另有<a href="https://cshihong.github.io/2020/04/18/BGP-in-the-datacenter-%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%9A%84BGP-%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84-Clos%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/">中文翻译版本</a></p></blockquote><h2 id="BIRD"><a href="#BIRD" class="headerlink" title="BIRD"></a>BIRD</h2><p>BIRD实际上是BIRD Internet Routing Daemon的缩写（禁止套娃），是一款可运行在Linux和其他类Unix系统上的路由软件，它实现了多种路由协议，比如BGP、OSPF、RIP等。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>BIRD会在内存中维护许多<strong>路由表</strong>，路由表根据不同的<strong>协议</strong>，通过与各种“其他事物”交换路由信息，来更新路由规则。这里说的“其他事物”可能是其他的路由表，也可能是外部的路由器，还可以是内核的某些API。</p><h3 id="路由表"><a href="#路由表" class="headerlink" title="路由表"></a>路由表</h3><p>路由表（Routing tables）是BIRD的核心，一个路由表是内存中一组路由规则的集合，BIRD根据网络类型的不同会有多种路由表。默认情况下，BIRD有<code>master4</code>和<code>master6</code>两个默认的路由表，分别保存IPv4和IPv6路由规则。除此外，你也可以创建其他的路由表，比如在配置文件<code>bird.conf</code>中添加如下配置，创建一个IPv4的路由表<code>my_table</code>。</p><pre><code class="hljs text">ipv4 table my_table;</code></pre><p>要注意的是，BIRD的路由表仅仅是一个表，并没有转发的功能，真正的转发控制，是内核的FIB（Forwarding Information Base）。而BIRD的<code>kernel</code>协议，可以将BIRD路由表与FIB进行同步，后面会介绍。</p><p>路由规则中包含了各种<strong>路由属性</strong>（Route attributes），网络类型不同的路由表，其路由属性也不太一样，比如常见的IPv4和IPv6的路由表，会包括两个路由属性：</p><ul><li>路由目的地</li><li>路由下一跳</li></ul><p>而VPN路由表还会包含路由属性：路由标识符（Route distinguisher）。</p><p>BIRD的每种表都会将一个或一组路由属性作为<strong>主键</strong>，类似于SQL数据库。当多个来源都提供了相同主键的路由条目时，BIRD会根据一定的规则选择最优路由。例如IPv4和IPv6类型的路由表，将“路由目的地”作为主键。</p><h3 id="协议与通道"><a href="#协议与通道" class="headerlink" title="协议与通道"></a>协议与通道</h3><p>协议（Protocols）将路由表和“其他事物”连接起来。“其他事物”可以是一个Socket对象，连接了外部的路由器，例如BGP路由协议；也可以是修改FIB的内核API，例如<code>kernel</code>协议；也可以是空，比如静态路由<code>static</code>协议。一个协议可以实例化为多个对象，例如创建多个BGP协议的实例，以表示多个BGP邻居。</p><p>协议也会提供一些路由属性，根据协议的不同路由属性也不同，比如使用BGP协议时，会有<code>bgp_path</code>属性。</p><p>协议可能包含一些<strong>通道</strong>（Channels），通道是在协议和路由表之间，配置了路由规则在导入（import)）、导出（export）两个方向上的行为，导入导出是针对路由表来说的，路由规则经过通道后，或是被接收（Accept），或是被拒绝（Reject），或是被修改。不同的协议可拥有的通道也不一样，例如BGP协议可以同时拥有<code>IPv4</code>和<code>IPv6</code>通道，RIP只能拥有IPv4或IPv6一种协议，而BFD则没有通道。</p><p>下面是根据官网样例修改而来的配置，实例化了一个名为<code>peer_one</code>的BGP协议，并且设置了<code>ipv4</code>和<code>ipv6</code>两个通道，两个通道都未指明连接的路由表，则使用默认的<code>master4</code>与<code>master6</code>路由表。其中在<code>ipv4</code> 通道中，导入方向配置为全部接收，导出方向上只导出静态路由，同时还会对路由规则的BGP信息进行修改：修改bgp  community，修改bgp  path；在<code>ipv6</code>通道上，则直接使用默认配置。</p><pre><code class="hljs json">protocol bgp peer_one &#123;        local 198.51.100.14 as 65000;        # Use a private AS number        neighbor 198.51.100.130 as 64496;    # Our neighbor ...        ipv4 &#123;                export filter &#123;                      # We use non-trivial export rules                        if source = RTS_STATIC then &#123; # Export only static routes                                # Assign our community                                bgp_community.add((65000,64501));                                # Artificially increase path length                                # by advertising local AS number twice                                if bgp_path ~ [= 65000 =] then                                        bgp_path.prepend(65000);                                accept;                        &#125;                        reject;                &#125;;                import all;        &#125;;        ipv6;&#125;</code></pre><h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><h3 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h3><p>在BIRD中，可以定义<strong>模板</strong>（template），通过模板来创建一个协议的多个实例。模板在使用BGP协议时非常好用，因为BGP通常都会设置多个BGP Peer。例如下面配置，通过模板提取出共用的配置，然后利用模板创建多个BGP邻居。</p><pre><code class="hljs json">template bgp foo &#123;        local 198.51.100.14 as 65000;        ipv4 &#123;                table mytable4;                import filter &#123; ... &#125;;                export none;        &#125;;        ipv6 &#123;                table mytable6;                import filter &#123; ... &#125;;                export none;        &#125;;&#125;protocol bgp bgp1 from foo &#123;        neighbor 198.51.100.130 as 64496;&#125;protocol bgp bgp2 from foo &#123;        neighbor 198.51.100.131 as 64496;&#125;</code></pre><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p><strong>过滤器</strong>（Filters）在上面的用例中已经出现了多次，通过在通道中添加过滤器，可以灵活地控制路由规则的交换。在过滤器中，你可以像访问变量一样直接使用各种路由属性，来编写各种判断条件，以决定对路由规则是ACCEPT还是REJECT，或是直接对路由属性进行修改。</p><p>为了便于复用，还可以以函数的形式定义一个过滤器，使用时在相应的通道中直接调用。需要注意的是，编写过滤器需要使用BIRD提供的专门的编程语言，它提供了一些例如<code>if</code>、<code>switch</code>的简单控制结构，但不允许有循环出现。同时，除了<code>int</code>、<code>string</code>这些基础的数据结构外，它还提供了例如<code>bgppatch</code>、<code>bgpmask</code>等这种表示路由规则中某些信息的数据结构。例如下面定义了一个名为<code>not_too_far</code>的过滤器，丢弃掉<code>rip_metric</code>大于10的路由规则，可以通过<code>import filter not_too_far</code>直接调用此函数。</p><pre><code class="hljs C">filter not_too_far<span class="hljs-keyword">int</span> var;&#123;        <span class="hljs-function"><span class="hljs-keyword">if</span> <span class="hljs-title">defined</span><span class="hljs-params">( rip_metric )</span> then</span><span class="hljs-function">                var </span>= rip_metric;        <span class="hljs-keyword">else</span> &#123;                var = <span class="hljs-number">1</span>;                rip_metric = <span class="hljs-number">1</span>;        &#125;        <span class="hljs-keyword">if</span> rip_metric &gt; <span class="hljs-number">10</span> then                reject <span class="hljs-string">&quot;RIP metric is too big&quot;</span>;        <span class="hljs-keyword">else</span>                accept <span class="hljs-string">&quot;ok&quot;</span>;&#125;</code></pre><h2 id="常见协议"><a href="#常见协议" class="headerlink" title="常见协议"></a>常见协议</h2><p>这里只介绍Calico中使用到的几种协议，以及用到的协议属性。</p><h3 id="device"><a href="#device" class="headerlink" title="device"></a>device</h3><p>准确来说，<code>device</code>并不算是一个协议，它不产生任何路由，也不支持通道，而是被用来从内核中获取网卡设备的信息。每个<code>bird.conf</code>的配置文件中，都应定义一个<code>device</code>。</p><pre><code class="hljs json">protocol device &#123;        scan time 10;           # Scan the interfaces often        interface &quot;eth0&quot; &#123;                preferred 192.168.1.1;                preferred 2001:db8:1:10::1;        &#125;;&#125;</code></pre><p>上面配置定义了BIRD每10s扫描一遍<code>eth0</code>网卡，同时定义了首选的IP地址。</p><h3 id="kernel"><a href="#kernel" class="headerlink" title="kernel"></a>kernel</h3><p><code>kernel</code>也不算真正的协议，它负责同步路由表与内核。如果内核支持多个内核路由表，那么可以创建多个<code>kernel</code>实例，否则只需要创建一个<code>kernel</code>实例。<code>kernel</code>协议有两个限制：</p><ul><li>不能将多个<code>kernel</code>实例都连接到同一个路由表上</li><li>不能修改导出（export）路由规则的目标地址</li></ul><p>一些主要的参数包括：</p><ul><li><code>learn switch</code>，开启后路由表可以从内核中学习到非内核生成（其他方式添加）的路由。“内核生成的路由”指的是由于本机网络的配置而产生的路由，比如<code>eth0</code>在被分配<code>192.168.1.2/24</code>后，会自动产生一条目的地为<code>192.168.1.0/24</code>，下一跳为<code>eth0</code>的路由。需要注意的是，即使是开启<code>learn</code>，<code>kernel</code>也不会将这些路由从内核导入（import）路由表，这种路由的传递需要使用到<code>direct</code>协议。（<code>switch</code>表示<code>on</code>和<code>off</code>两种值，下面相同）</li><li><code>persist switch</code>，BIRD退出时，在内核保留同步的路由（即不会进行clean up操作）。</li><li><code>scan time number</code>，同步间隔，单位秒</li></ul><pre><code class="hljs json"># 同步master4、master6路由表与主FIB，并在退出后保持同步的路由protocol kernel &#123;    learn;    persist;&#125;</code></pre><h3 id="direct"><a href="#direct" class="headerlink" title="direct"></a>direct</h3><p>如上面所述，<code>direct</code>用于将内核生成的路由规则从内核导入到BIRD路由表中，可用的参数包括：</p><ul><li><code>interface pattern [, ...]</code>，用于指定传递由哪些网卡生成的路由规则，默认是全部网卡</li><li><code>check link switch</code>，开启后会考虑link的状态，当link状态为up时，传递路由，否则，撤销传递的路由</li></ul><pre><code class="hljs json"># 同步除了eth0以外的其他网卡protocol direct &#123;  interface -&quot;eth0&quot;, &quot;*&quot;; &#125;</code></pre><h3 id="BGP"><a href="#BGP" class="headerlink" title="BGP"></a>BGP</h3><p>每一个BGP协议的实例，代表了一个BGP Peer连接。需要注意的是，部分参数的默认值对IBGP与EBGP并不相同，例如<code>aigp</code>默认在IBGP中是开启的，默认在EBGP是关闭的。协议的主要参数包括：</p><ul><li><code>local [ip] [port number] [as number]</code>，可以用来指定BGP的源IP地址以及本地的AS。</li><li><code>multihop [number]</code>，表示多跳的BGP，后面的<code>number</code>可以用来设置<code>TTL</code>的值，IBGP默认开启；相反的，还有个参数为<code>direct</code>，表示与BGP邻居直连，EBGP默认开启。</li><li><code>source address ip</code>，用来指定本端使用的BGP源地址。</li><li><code>add paths switch|rx|tx</code>，开启时，会将 BGP 配置为向同一目标通告多个路径，否则 BGP 仅通告活动路径。</li><li><code>password string</code> ，使用设置的密码进行BGP的身份验证。</li><li><code>rr client</code>，开启RR模式（Route Reflector）。</li><li><code>rr cluster id IPv4 address</code>，设置RR的<code>cluster id</code>，以防止路由环路。默认情况下，会直接使用BGP的<code>router id</code>（一般是ipv4地址）作为<code>cluster id</code>，当有多个RR时，需要使用此参数设置相同的<code>cluster id</code>。</li><li><code>bfd switch|graceful</code>，使用BFD作为BGP协议心跳机制。</li><li><code>passive switch</code>，被动模式，不主动初始化连接，而是等待其他BGP邻居发起连接。</li></ul><p>以上是协议一层的配置参数，在BGP协议中，通道也会有额外的参数，例如：</p><ul><li><code>gateway direct|recursive</code>，用来控制如何计算路由的gw属性。当设置为<code>direct</code>时，如果路由中的<code>bgp_next_hop</code>是和本机中的某个地址同一子网（同一个二层），则gw直接为<code>bgp_next_hop</code>，否则为对端BGP Peer的IP地址；当设置为<code>recursive</code>时，会从IGP路由表中查询<code>bgp_next_hop</code>来作为gw。</li><li><code>next hop keep switch|ibgp|ebgp</code>，开启后，BGP不再将<code>next hop</code>属性修改为自身，而是直接通告原始的<code>next hop</code>，这个参数在多跳的EBGP场景或BGP路由反射中会使用到。</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://bird.network.cz/?get_doc&v=20&f=bird.html#toc5">BIRD官方手册</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>calico</tag>
      
      <tag>BGP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深入kube-proxy ipvs模式的conn_reuse_mode问题</title>
    <link href="/2021/01/15/%E6%B7%B1%E5%85%A5kube-proxy%20ipvs%E6%A8%A1%E5%BC%8F%E7%9A%84conn_reuse_mode%E9%97%AE%E9%A2%98/"/>
    <url>/2021/01/15/%E6%B7%B1%E5%85%A5kube-proxy%20ipvs%E6%A8%A1%E5%BC%8F%E7%9A%84conn_reuse_mode%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>在高并发、短连接的场景下，kube-proxy ipvs存在rs删除失败或是延迟高的问题，社区也有不少Issue反馈，比如<a href="https://github.com/kubernetes/kubernetes/issues/81775">kube-proxy ipvs conn_reuse_mode setting causes errors with high load from single client</a>。文本对这些问题进行了梳理，试图介绍产生这些问题的内部原因。由于能力有限，其中涉及内核部分，只能浅尝辄止。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="端口重用"><a href="#端口重用" class="headerlink" title="端口重用"></a>端口重用</h3><p>一切问题来源于端口重用。在TCP四次挥手中有个<code>TIME_WAIT</code>的状态，作为先发送<code>FIN</code>包的一端，在接收到对端发送的<code>FIN</code>包后进入<code>TIME_WAIT</code>，在经过<code>2MSL</code>后才会真正关闭连接。<code>TIME_WAIT</code>状态的存在，一来可以避免将之前连接的延迟报文，作为当前连接的报文处理；二是可以处理最后一个ACK丢失带来的问题。</p><p><img src="/img/2021-01-15-TCP.png"></p><p>而在短连接、高并发的场景下，会出现大量的<code>TIME-WAIT</code>连接，导致资源无法及时释放。Linux中内核参数<code>net.ipv4.tcp_tw_reuse</code>提供了一种减少<code>TIME-WAIT</code>连接的方式，可以将<code>TIME-WAIT</code>连接的端口分配给新的TCP连接，来复用端口。</p><pre><code class="hljs text">tcp_tw_reuse - BOOLEANAllow to reuse TIME-WAIT sockets for new connections when it issafe from protocol viewpoint. Default value is 0.It should not be changed without advice/request of technicalexperts.</code></pre><h3 id="ipvs如何处理端口重用？"><a href="#ipvs如何处理端口重用？" class="headerlink" title="ipvs如何处理端口重用？"></a>ipvs如何处理端口重用？</h3><p>ipvs对端口的复用策略主要由内核参数<code>net.ipv4.vs.conn_reuse_mode</code>决定</p><pre><code class="hljs text">conn_reuse_mode - INTEGER1 - defaultControls how ipvs will deal with connections that are detectedport reuse. It is a bitmap, with the values being:0: disable any special handling on port reuse. The newconnection will be delivered to the same real server that wasservicing the previous connection. This will effectivelydisable expire_nodest_conn.bit 1: enable rescheduling of new connections when it is safe.That is, whenever expire_nodest_conn and for TCP sockets, whenthe connection is in TIME_WAIT state (which is only possible ifyou use NAT mode).bit 2: it is bit 1 plus, for TCP connections, when connectionsare in FIN_WAIT state, as this is the last state seen by loadbalancer in Direct Routing mode. This bit helps on adding newreal servers to a very busy cluster.</code></pre><p>当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上；当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，ipvs则会对新连接进行重新调度。</p><p>相关的，还有一个内核参数<code>net.ipv4.vs.expire_nodest_conn</code>，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包<strong>silently drop</strong>，也就是DROP掉数据包但不结束连接，等待客户端的重试。</p><p>另外，关于<strong>destination 不可用</strong>的判断，是在ipvs执行删除<code>vs</code>（在<code>__ip_vs_del_service()</code>中实现）或删除<code>rs</code>（在<code>ip_vs_del_dest()</code>中实现）时，会调用<code>__ip_vs_unlink_dest()</code>方法，将相应的destination置为不可用。</p><pre><code class="hljs text">expire_nodest_conn - BOOLEAN        0 - disabled (default)        not 0 - enabled        The default value is 0, the load balancer will silently drop        packets when its destination server is not available. It may        be useful, when user-space monitoring program deletes the        destination server (because of server overload or wrong        detection) and add back the server later, and the connections        to the server can continue.        If this feature is enabled, the load balancer will expire the        connection immediately when a packet arrives and its        destination server is not available, then the client program        will be notified that the connection is closed. This is        equivalent to the feature some people requires to flush        connections when its destination is not available.</code></pre><p>  关于ipvs如何处理端口复用的连接，这块主要实现逻辑在<code>net/netfilter/ipvs/ip_vs_core.c</code>的<code>ip_vs_in()</code>方法中：</p><pre><code class="hljs C"><span class="hljs-comment">/*</span><span class="hljs-comment"> * Check if the packet belongs to an existing connection entry</span><span class="hljs-comment"> */</span>cp = pp-&gt;conn_in_get(ipvs, af, skb, &amp;iph);  <span class="hljs-comment">//找是属于某个已有的connection</span>conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);<span class="hljs-comment">//当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的connection），进行处理</span><span class="hljs-keyword">if</span> (conn_reuse_mode &amp;&amp; !iph.fragoffs &amp;&amp; is_new_conn(skb, &amp;iph) &amp;&amp; cp) &#123; <span class="hljs-keyword">bool</span> uses_ct = <span class="hljs-literal">false</span>, resched = <span class="hljs-literal">false</span>;<span class="hljs-comment">//如果开启了expire_nodest_conn、目标rs的weight为0</span><span class="hljs-keyword">if</span> (unlikely(sysctl_expire_nodest_conn(ipvs)) &amp;&amp; cp-&gt;dest &amp;&amp;    unlikely(!atomic_read(&amp;cp-&gt;dest-&gt;weight))) &#123;resched = <span class="hljs-literal">true</span>;<span class="hljs-comment">//查询是否用到了conntrack</span>uses_ct = ip_vs_conn_uses_conntrack(cp, skb);&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (is_new_conn_expected(cp, conn_reuse_mode)) &#123;<span class="hljs-comment">//连接是expected的情况，比如FTP</span>uses_ct = ip_vs_conn_uses_conntrack(cp, skb);<span class="hljs-keyword">if</span> (!atomic_read(&amp;cp-&gt;n_control)) &#123;resched = <span class="hljs-literal">true</span>;&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">/* Do not reschedule controlling connection</span><span class="hljs-comment"> * that uses conntrack while it is still</span><span class="hljs-comment"> * referenced by controlled connection(s).</span><span class="hljs-comment"> */</span>resched = !uses_ct;&#125;&#125;<span class="hljs-comment">//如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了</span><span class="hljs-keyword">if</span> (resched) &#123;<span class="hljs-keyword">if</span> (!atomic_read(&amp;cp-&gt;n_control))ip_vs_conn_expire_now(cp);__ip_vs_conn_put(cp);<span class="hljs-comment">//当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN</span><span class="hljs-keyword">if</span> (uses_ct)<span class="hljs-keyword">return</span> NF_DROP;<span class="hljs-comment">//未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程</span>cp = <span class="hljs-literal">NULL</span>;&#125;&#125;<span class="hljs-keyword">if</span> (unlikely(!cp)) &#123;<span class="hljs-keyword">int</span> v;<span class="hljs-keyword">if</span> (!ip_vs_try_to_schedule(ipvs, af, skb, pd, &amp;v, &amp;cp, &amp;iph))<span class="hljs-keyword">return</span> v;&#125;IP_VS_DBG_PKT(<span class="hljs-number">11</span>, af, pp, skb, iph.off, <span class="hljs-string">&quot;Incoming packet&quot;</span>);<span class="hljs-comment">/* Check the server status */</span><span class="hljs-keyword">if</span> (cp-&gt;dest &amp;&amp; !(cp-&gt;dest-&gt;flags &amp; IP_VS_DEST_F_AVAILABLE)) &#123;<span class="hljs-comment">/* the destination server is not available */</span>__u32 flags = cp-&gt;flags;<span class="hljs-comment">/* when timer already started, silently drop the packet.*/</span><span class="hljs-keyword">if</span> (timer_pending(&amp;cp-&gt;timer))__ip_vs_conn_put(cp);<span class="hljs-keyword">else</span>ip_vs_conn_put(cp);<span class="hljs-keyword">if</span> (sysctl_expire_nodest_conn(ipvs) &amp;&amp;    !(flags &amp; IP_VS_CONN_F_ONE_PACKET)) &#123;<span class="hljs-comment">/* try to expire the connection immediately */</span>ip_vs_conn_expire_now(cp);&#125;<span class="hljs-keyword">return</span> NF_DROP;&#125;</code></pre><h3 id="kube-proxy-ipvs模式下的优雅删除"><a href="#kube-proxy-ipvs模式下的优雅删除" class="headerlink" title="kube-proxy ipvs模式下的优雅删除"></a>kube-proxy ipvs模式下的优雅删除</h3><p>Kubernetes提供了Pod优雅删除机制。当我们决定干掉一个Pod时，我们可以通过<code>PreStop Hook</code>来做一些服务下线前的处理，同时Kubernetes也有个<code>grace period</code>，超过这个时间但未完成删除的Pod会被强制删除。</p><p>而在Kubernetes 1.13之前，kube-proxy ipvs模式并不支持优雅删除，当Endpoint被删除时，kube-proxy会直接移除掉ipvs中对应的rs，这样会导致后续的数据包被丢掉。</p><p>在1.13版本后，Kubernetes添加了<a href="https://github.com/kubernetes/kubernetes/pull/66012">IPVS优雅删除</a>的逻辑，主要是两点：</p><ul><li>当Pod被删除时，kube-proxy会先将rs的<code>weight</code>置为0，以防止新连接的请求发送到此rs，由于不再直接删除rs，旧连接仍能与rs正常通信；</li><li>当rs的<code>ActiveConn</code>数量为0（后面版本已改为<code>ActiveConn+InactiveConn==0</code>)，即不再有连接转发到此rs时，此rs才会真正被移除。</li></ul><h2 id="kube-proxy-ipvs模式下的问题"><a href="#kube-proxy-ipvs模式下的问题" class="headerlink" title="kube-proxy ipvs模式下的问题"></a>kube-proxy ipvs模式下的问题</h2><p>看上去kube-proxy ipvs的删除是优雅了，但当优雅删除正巧碰到端口重用，那问题就来了。</p><p>首先，kube-proxy希望通过设置<code>weight</code>为0，来避免新连接转发到此rs。但当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，对于端口复用的连接，ipvs不会主动进行新的调度（调用<code>ip_vs_try_to_schedule</code>方法）；同时，只是将<code>weight</code>置为0，也并不会触发由<code>expire_nodest_conn</code>控制的结束连接或DROP操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的Pod。这样一来，只要不断的有端口复用的连接请求发来，rs就不会被kube-proxy删除，上面提到的优雅删除的两点均无法实现。</p><p>而当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，根据<code>ip_vs_in()</code>的处理逻辑，当开启了<code>net.ipv4.vs.conntrack</code>时，会DROP掉第一个SYN包，导致SYN的重传，有1S延迟。而Kube-proxy在IPVS模式下，使用了iptables进行<code>MASQUERADE</code>，也正好开启了<code>net.ipv4.vs.conntrack</code>。</p><pre><code class="hljs text">conntrack - BOOLEAN0 - disabled (default)not 0 - enabledIf set, maintain connection tracking entries forconnections handled by IPVS.This should be enabled if connections handled by IPVS are to bealso handled by stateful firewall rules. That is, iptables rulesthat make use of connection tracking.  It is a performanceoptimisation to disable this setting otherwise.Connections handled by the IPVS FTP application modulewill have connection tracking entries regardless of this setting.Only available when IPVS is compiled with CONFIG_IP_VS_NFCT enabled.</code></pre><p>这样看来，目前的情况似乎是，如果你需要实现优雅删除中的“保持旧连接不变，调度新连接”能力，那就要付出1s的延迟代价；如果你要好的性能，那么就不能重新调度。</p><h2 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h2><p>从Kubernetes角度来说，Kube-proxy需要在保证性能的前提下，找到一种能让新连接重新调度的方式。但目前从内核代码中可以看到，需要将参数设置如下</p><pre><code class="hljs text">net.ipv4.vs.conntrack=0net.ipv4.vs.conn_reuse_mode=1net.ipv4.vs.expire_nodest_conn=1</code></pre><p>但Kube-proxy ipvs模式目前无法摆脱iptables来完成k8s service的转发。此外，Kube-proxy只有在<code>ActiveConn+InactiveConn==0</code>时才会删除rs，除此之外，在新的Endpoint和<code>GracefulTerminationList</code>（保存了<code>weight</code>为0，但暂未删除的rs）中的rs冲突时，才会立即删除rs。这种逻辑似乎并不合理。目前Pod已有优雅删除的逻辑，而kube-proxy应基于Pod的优雅删除，在网络层面做好rs的优雅删除，因此在kubelet完全删除Pod后，Kube-proxy是否也应该考虑同时删除相应的rs？</p><p>另外，从内核角度来说，ipvs需要提供一种方式，能在端口复用、同时使用conntrack的场景下，可以对新连接直接重新调度。</p><h2 id="即将到来"><a href="#即将到来" class="headerlink" title="即将到来"></a>即将到来</h2><p>这个问题在社区讨论一段时间后，目前出现的几个相关的解决如下：<br><strong>内核两个Patch</strong></p><ul><li><a href="http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200701151719.4751-1-ja@ssi.bg/">ipvs: allow connection reuse for unconfirmed conntrack</a><br>修改了<code>ip_vs_conn_uses_conntrack()</code>方法的逻辑，当使用<code>unconfirmed conntrack</code>时，返回false，这种修改针对了TIME_WAIT的conntrack。</li><li><a href="http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200708161638.13584-1-kim.andrewsy@gmail.com/"> ipvs: queue delayed work to expire no destination connections if expire_nodest_conn=1</a><br>提前了<code>expire  connection</code>的操作，在destination被删除后，便开始将<code>expire  connection</code>操作入队列。而不是等到数据包真正发过来时，才做<code>expire  connection</code>，以此来减少数据包的丢失。</li></ul><p><strong>Kubernetes</strong><br><a href="https://github.com/kubernetes/enhancements/pull/1607">Graceful Termination for External Traffic Policy Local</a><br><a href="https://github.com/kubernetes/kubernetes/pull/92968">Add Terminating Condition to EndpointSlice</a><br>正如前面所说的，Kube-proxy需要能够感知到Pod的优雅删除过程，来同步进行rs的删除。目前，已有一个相应的KEP在进行中，通过在<code>Endpoint.EndpointConditions</code>中添加<code>terminating</code>字段，来为kube-proxy提供感知方式。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kube-proxy</tag>
      
      <tag>ipvs</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
